在实际的开发中，我们会用一些集群服务器进行服务的部署，在不同的公司，这些环境可能被称为预发布环境或是泳道等。

通常在上线前，我们会对这些小流量环境进行预估流量的压测，来预估目前的小流量集群服务器能否承载对应的流量，进而评估一下，我们使用多少服务器集群部署服务，才能足够承载流量，又不至于浪费服务器资源。

在字节内部，我们有一套闭环的压测链路解决方案，来保证整个流程的维稳，但是有一些小公司或是个体团队的同学可能并没有压测等平台，也没有专业的测开同学可以协助进行分析集群分析，有没有什么 MVP 的方案可以进行简单快捷的方式进行压测，又可以得到我们需要的数据呢？

通常针对并发的压测，业内推荐的方案大部分是 Jmeter，它是一款基于 Java 实现，支持接口并发、模拟协议请求等能力的测试工具。不过本节课我们不使用 Jmeter 来进行压测，一方面它安装上不够轻量，因为基于 Java 实现，所以我们需要配置 Java 的环境变量；另一方面，Jmeter 提供了很多额外的能力，包括功能测试和回归测试等，我们这节课的核心需求是对官网应用“并发量”进行判定，这些功能我们都是用不上的。

我们来对比一下三个常见的开源方案，都是比较轻量且适合开发使用的压测工具，需要注意的是，本节课的压测是基于本地服务压测，**对于实际上线，需要先部署在测试服务器，然后对测试环境内网域名进行压测，进而判断能否承受预估的QPS，从而对服务集群进行扩容等操作。**

***Ps：不清楚怎么部署的同学可以移步到[20 | 应用上线: 从官网服务域名的申请备案到服务部署](https://juejin.cn/book/7137945369635192836/section/7141558366945411083)*** ***进行学习。***

## WebBench

> https://github.com/EZLippi/WebBench
>
> Webbench是一个在 Linux 下使用的非常简单的网站压测工具。它使用 fork() 模拟多个客户端同时访问我们设定的 URL，测试网站在压力下工作的性能，最多可以模拟 3 万个并发连接去测试网站的负载能力。

WebBench 不能支持 Windows，只能在 Linux 等类 UNIX 系统下使用，我们以 macOS 系统为例安装试试。

首先我们需要安装一下 brew，这是一个针对 macOS 和 Linux 的包管理工具，终端里直接执行下面的命令就好，很多使用 mac 系统的同学可能已经装过这个了，安装完在终端里直接输入 brew 看下有没有正常输出即可。

```
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

然后我们装一下 wget，它是 Linux 下的一个安装文件的工具，对应的安装包我们可以通过它下载下来。

```
brew install wget
```

最后我们来装一下 WebBench。

```
wget http://www.ha97.com/code/webbench-1.5.tar.gz
tar zxvf webbench-1.5.tar.gz // 解压
cd webbench-1.5
make
make install
```

安装完以后，我们可以在终端中输入 WebBench 验证一下。


![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e1f625c9fed44ca583afac252f22f168~tplv-k3u1fbpfcp-watermark.image?)

我们需要关注的参数有两个：

-   -c: 并发量；

<!---->

-   -t: 运行时间。

我们对我们的服务简单压测试验下看看。


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/055cc6ec227d4d24887de7f309689ce8~tplv-k3u1fbpfcp-watermark.image?)

可以看到 200 并发，咱们本地服务是 hold 住的， 2000 并发就会出现大规模请求异常的情况，不过这个结果算比较简陋的，加上对环境和安装步骤上相对苛刻一些，所以我并不推荐大家使用这个方案。

## wrk

> https://github.com/wg/wrk
>
> wrk 是一款针对 HTTP 协议的基准测试工具，它能够在单机多核 CPU 的条件下，使用系统自带的高性能 I/O 机制，如 epoll、kqueue 等，通过多线程和事件模式，对目标机器产生大量的负载。

wrk 是一款轻量级的性能测试工具，支持大多数类 UNIX 系统，不支持 Windows。不同的类 UNIX 系统安装方式也略有差异，下面以 macOS 系统为例。

我们来装一下 wrk。

```
brew install wrk
```

装完可以在终端执行一下 `wrk -v`验证一下。


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7927dcc8e04e41fcaabdc500ea513434~tplv-k3u1fbpfcp-watermark.image?)

上面执行完以后我们可以看到它列出了 wrk相关的参数，其中我们常用到的有三个参数：

-   -c: 保持打开状态的 HTTP 连接总数；

<!---->

-   -d: 测试时长；

<!---->

-   -t: 使用线程。

其中连接数（c）会平分给每个线程，比如设置 -c200 -t8，那么将启用 8 个线程，每个线程处理 200/8 个请求，我们可以对 bing 搜索简单试验一下，具体参数其实大部分都是一样的，我们在 autocannon 的部分统一介绍。


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0107c07b42c541329c02b4b72d72b514~tplv-k3u1fbpfcp-watermark.image?)

其实，这个方案更多是给后端同学测吞吐率用的，包括线程等参数，具体的值不好衡量，对前端不算那么友好，感兴趣的同学可以下来试试看。

## autocannon

> https://github.com/mcollina/autocannon
>
> 一个用 Node 编写的 HTTP/1.1 基准测试工具，受到 wrk 和 wrk2 的极大启发，支持 HTTP 管道和 HTTPS。autocannon 可以产生比 wrk 和 wrk2 更多的负载。

autocannon 是用 Node 编写的 HTTP/1.1 基准测试工具，它可以同时支持 Windows、 Mac 和 Linux 的 环境，而且作为 一个 npm 包，使用上比较符合前端的开发习惯，安装更为方便，使用方式也很轻量，很推荐大家使用这种方式。

```
npm i autocannon -g
```

Autocannon 有提供一些参数来对应不同压测指数，我们常用的有 3 个：

-   -c: 要使用的并发连接数。默认值：10 ；

<!---->

-   -p: 使用流水线请求的数量。默认值：1 ；

<!---->

-   -d: 运行秒数。默认值：10 。

同样，我们来对我们的服务做一个简单尝试，首先测试一下默认值的效果。

```
autocannon http://127.0.0.1:3000
```

在这个 10s 的执行过程，我们如果切回 nextjs-demo 是可以看到我们的服务在飞快请求的。


![dffa36a3-bfcb-422f-8de4-8cebab33d848.gif](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7446fc63a6584fd4b3813ca13c28f4ad~tplv-k3u1fbpfcp-watermark.image?)

最后我们可以得到这样一个数据。


![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/67ffccf77d3b4ace869e8c45ae12b768~tplv-k3u1fbpfcp-watermark.image?)

这个表格怎么看呢，我给大家介绍一下每个指标对应啥，我们先看每列的指标：

-   2.5% / 50% / 97.5% / 99%：整个过程百分比所对应的值；

<!---->

-   Avg: 平均值；

<!---->

-   Stdev: 标准差；

<!---->

-   Max: 最大值。

对于每行的指标含义是这样的：

-   Latency: 耗时(毫秒)；

<!---->

-   Req/Sec: QPS，吞吐量，每秒请求数；

<!---->

-   Bytes/Sec: 每秒请求字节数。

这些指标通常在对具体接口或是页面 case by case 的性能分析中会有使用，服务器资源判定我们只需要关注请求时间是否过长，或是是否存在大面积报错即可，这里我们可以看到大部分数值是正常的，也没有报错等信息。

接下来我们把并发量提高到 200， 再来看下效果，可以看到我们的服务仍然是可以 hold 住的。

```
autocannon -c 200 http://127.0.0.1:3000
```


![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a2eaa42ea8d474791cc32d7b76b6faa~tplv-k3u1fbpfcp-watermark.image?)

接下来，我们把并发量提高到 2000，再看看会得到怎样的结果。


![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7807c986317941219a4722325292ab52~tplv-k3u1fbpfcp-watermark.image?)

从数据上看，我们发现所有的数据都清 0 了，说明在这个并发量下单服务器的计算支撑不下去，最下面的请求数据中也有显示 3k 个错误， 2k 个超时。

这时候我们切回 nextjs-demo 的终端可以看到，我们的服务已经崩掉了，没办法承载 2000 的并发量，如果业务需要，这时候就需要考虑给服务器集群进行扩容操作了。


![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a858f44ed55149aeb297663d09488924~tplv-k3u1fbpfcp-watermark.image?)

## 小结

这节课我们学习了怎么对官网应用进行上线前压测，通常大公司可能会有自己的一套压测解决方案，对于没有压测资源的同学可以采用一些轻量的开源工具来对服务进行压力测试。

我们对比了 WebBench、 wrk、 autocannon 三种开源方案，其中 WebBench 和 wrk 只能用于 Linux 环境，而且压测的信息和配置不够轻量，相比之下，我更推荐大家使用 autocannon 的方案，它基于 nodejs 开发，具备更符合我们开发习惯的配置和使用方式。

我们通过 autocannon 对服务进行了多个并发量的压测对比，分析出在 2000 并发下，我们的服务是支撑不住的，需要通过集群扩容等方式支持。

目前我们还只是对本地服务进行测试，正常的上线流程中，大家需要先去申请测试服务器的资源，在测试服务器上压测评估后再部署线上服务器。

我们的官网在上线后，常常需要取用户操作的数据进行 pv 等相关分析，这时候我们就需要进行一些基础的数据埋点，并且进行统计分析，下节课我们就来学习如何对用户的数据进行上报分析。
